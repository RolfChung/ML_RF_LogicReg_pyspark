{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "<p>\n",
    "This project is doing explorative data analysis using the pyspark and sql application programming interfaces (API's) of Apache spark.\n",
    "</p> \n",
    "\n",
    "<p>\n",
    "Apache Spark is an open-source engine developed specifically for handling large-scale data processing and analytics. Spark offers the ability to access data in a variety of sources. Apache Spark is designed to accelerate analytics on Hadoop with speed and efficiency.\n",
    "</p> \n",
    "<a href=\"https://www.webopedia.com/TERM/A/apache-spark.html\" target=\"_blank\">webopedia</a> \n",
    "\n",
    "<p>\n",
    "The airtraffic system data records consist of the tables flights, planes,\n",
    "and airports. The data sources used  are csv-files stored locally. \n",
    "The functions and methods applied here include SQL-queries and\n",
    "SQL-calculations and pyspark implementations like select, filter,\n",
    "collect, join, and aggregate. \n",
    "</p> \n",
    "\n",
    "<p>\n",
    "Those can of course applied to Big Data on remote machines. This is the whole point of the Apache Spark system architecture. It allows even analysing streaming data in real time. The Spark master distributes then the data as Resilient Distributed Datasets (RDD) or immutable distributed collections of objects on the remote machines (workers) using a process of mapping, sort and shuffle, and reducing. RDD's are not generated in this project, but are in others.\n",
    "</p> \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"spark_architecture.jpg\" alt=\"Smiley face\" align=\"left\"  style=\"margin-left: 0px; margin-right: 0px; margin-top: 20px; margin-bottom: 20px; float: left; width: 800px; height: 300px\"> \n",
    "\n",
    "<p>\n",
    "The main focus of this project is prepare the data for machine learning for example by:\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "  <li>joining</li>\n",
    "  <li>creating labels</li>\n",
    "  <li>cleaning missing values</li>\n",
    "  <li>doing a train-test-split</li>\n",
    "</ul> \n",
    "\n",
    "\n",
    "<p>    \n",
    "and then apply Pyspark ML library alogrithms like:\n",
    "</p> \n",
    "\n",
    "<ul>\n",
    "  <li>Random Forest</li>\n",
    "  <li>Random Forest in cross-validation</li>\n",
    "  <li>Logistic Regression in cross-validation</li>\n",
    "  <li>Gradient-Boosted Tree Classifier</li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.functions import round, max, min, mean, stddev , col, avg \n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, \\\n",
    "StringIndexer, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.ml.evaluation as evaluation\n",
    "import pyspark.ml.tuning as tune\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from time import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd_1=os.getcwd()\n",
    "# print(cwd_1)\n",
    "os.chdir(r'C:\\Users\\gamarandor\\spark\\spark-3.0.0-preview-bin-hadoop2.7\\projects')\n",
    "cwd_2=os.getcwd()\n",
    "# print(cwd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Spark\n",
    "\n",
    "<p>\n",
    "\"A SparkSession is beyond a time-bounded interaction, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs. Most importantly, it curbs the number of concepts and constructs a developer has to juggle while interacting with Spark.\"\n",
    "</p> \n",
    "<a href=\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\" target=\"_blank\">Class SparkSession</a> \n",
    "\n",
    "<p>\n",
    "\"A SparkContext represents the connection to a Spark cluster, \n",
    "and can be used to create RDDs, accumulators and broadcast variables \n",
    "on that cluster.\" The SparkContext is the entry point into the cluster. Without creating a SparkContext nothing in the Spark session will work.\n",
    "</p> \n",
    "<a href=\"https://spark.apache.org/docs/2.0.2/api/java/org/apache/spark/SparkContext.html\" target=\"_blank\">Class SparkContext</a> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001F09530D748>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x000001F0975354C8>\n"
     ]
    }
   ],
   "source": [
    "## Create Spark context \n",
    "sqlContext = SQLContext (sc)\n",
    "print(sqlContext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "3.0.0-preview\n",
      "local[*]\n",
      "None\n",
      "gamarandor\n",
      "PySparkShell\n",
      "local-1578052098871\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)\n",
    "\n",
    "# Master URL to connect to\n",
    "print(sc.master)\n",
    "\n",
    "# Path where Spark is installed on worker nodes\n",
    "print(str(sc.sparkHome))\n",
    "\n",
    "# Retrieve name of the Spark User running\n",
    "print(str(sc.sparkUser()))\n",
    "\n",
    "# Return application name\n",
    "print(sc.appName)\n",
    "\n",
    "# Retrieve application ID\n",
    "print(sc.applicationId)\n",
    "\n",
    "# Return default level of parallelism \n",
    "print(sc.defaultParallelism)\n",
    "\n",
    "# Default minimum number of partitions for RDDs\n",
    "print(sc.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading csv-files into the Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[tailnum: string, year: string, type: string, manufacturer: string, model: string, engines: int, seats: int, speed: string, engine: string]\n"
     ]
    }
   ],
   "source": [
    "path_source = r\"planes.csv\"\n",
    "\n",
    "df_planes = sqlContext.read.load (path_source,\n",
    "                           format='com.databricks.spark.csv',\n",
    "                           header='true',\n",
    "                           inferSchema='true')\n",
    "                           \n",
    "print(df_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planes_csv\n"
     ]
    }
   ],
   "source": [
    "temp_table_name = \"planes_csv\"\n",
    "print(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_planes.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_flights = r\"flights_small.csv\"\n",
    "\n",
    "df_flights = sqlContext.read.load (src_flights,\n",
    "                           format='com.databricks.spark.csv',\n",
    "                           header='true',\n",
    "                           inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(year,IntegerType,true),StructField(month,IntegerType,true),StructField(day,IntegerType,true),StructField(dep_time,StringType,true),StructField(dep_delay,StringType,true),StructField(arr_time,StringType,true),StructField(arr_delay,StringType,true),StructField(carrier,StringType,true),StructField(tailnum,StringType,true),StructField(flight,IntegerType,true),StructField(origin,StringType,true),StructField(dest,StringType,true),StructField(air_time,StringType,true),StructField(distance,IntegerType,true),StructField(hour,StringType,true),StructField(minute,StringType,true)))\n",
      "\n",
      "df_flights\n"
     ]
    }
   ],
   "source": [
    "# schema delivers the data types of the variables or columns\n",
    "print(df_flights.schema)\n",
    "\n",
    "print(\"\")\n",
    "temp_table_name_2 = \"df_flights\"\n",
    "print(temp_table_name_2)\n",
    "df_flights.createOrReplaceTempView(temp_table_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(faa,StringType,true),StructField(name,StringType,true),StructField(lat,DoubleType,true),StructField(lon,DoubleType,true),StructField(alt,IntegerType,true),StructField(tz,IntegerType,true),StructField(dst,StringType,true)))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_airports =  r\"airports.csv\"\n",
    "\n",
    "df_airports = sqlContext.read.load (src_airports,\n",
    "                           format='com.databricks.spark.csv',\n",
    "                           header='true',\n",
    "                           inferSchema='true')\n",
    "df_airports.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_airports\n"
     ]
    }
   ],
   "source": [
    "temp_table_name_3 = \"df_airports\"\n",
    "print(temp_table_name_3)\n",
    "df_airports.createOrReplaceTempView(temp_table_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if object is a data frame or RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DataFrame'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_df(x):\n",
    "    if isinstance(x, RDD):\n",
    "        return \"RDD\"\n",
    "    if isinstance(x, DataFrame):\n",
    "        return \"DataFrame\"\n",
    "    \n",
    "check_df(df_airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession.catalog.listTables()\n",
    "<p>\n",
    "The attribute catalog of the active Spark session lists the data \n",
    "inside the cluster. The .listTables() method generates\n",
    "all the tables inside the cluster as a list. \n",
    "This helps to get orientation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='df_airports', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='df_flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='planes_csv', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.functions import mean, stddev , col, avg, round\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gamarandor\\spark\\spark-3.0.0-preview-bin-hadoop2.7\\projects\n",
      "C:\\Users\\gamarandor\\spark\\spark-3.0.0-preview-bin-hadoop2.7\\projects\n"
     ]
    }
   ],
   "source": [
    "cwd_1=os.getcwd()\n",
    "print(cwd_1)\n",
    "os.chdir(r'C:\\Users\\gamarandor\\spark\\spark-3.0.0-preview-bin-hadoop2.7\\projects')\n",
    "cwd_2=os.getcwd()\n",
    "print(cwd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Spark\n",
    "\n",
    "<p>\n",
    "\"A SparkSession is beyond a time-bounded interaction, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs. Most importantly, it curbs the number of concepts and constructs a developer has to juggle while interacting with Spark.\"\n",
    "</p> \n",
    "<a href=\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\" target=\"_blank\">Class SparkSession</a> \n",
    "\n",
    "<p>\n",
    "\"A SparkContext represents the connection to a Spark cluster, \n",
    "and can be used to create RDDs, accumulators and broadcast variables \n",
    "on that cluster.\" The SparkContext is the entry point into the cluster. Without creating a SparkContext nothing in the Spark session will work.\n",
    "</p> \n",
    "<a href=\"https://spark.apache.org/docs/2.0.2/api/java/org/apache/spark/SparkContext.html\" target=\"_blank\">Class SparkContext</a> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001F09530D748>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.context.SQLContext object at 0x000001F09753C648>\n"
     ]
    }
   ],
   "source": [
    "## Create Spark context \n",
    "sqlContext = SQLContext (sc)\n",
    "print(sqlContext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "3.0.0-preview\n",
      "local[*]\n",
      "None\n",
      "gamarandor\n",
      "PySparkShell\n",
      "local-1578052098871\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)\n",
    "\n",
    "# Master URL to connect to\n",
    "print(sc.master)\n",
    "\n",
    "# Path where Spark is installed on worker nodes\n",
    "print(str(sc.sparkHome))\n",
    "\n",
    "# Retrieve name of the Spark User running\n",
    "print(str(sc.sparkUser()))\n",
    "\n",
    "# Return application name\n",
    "print(sc.appName)\n",
    "\n",
    "# Retrieve application ID\n",
    "print(sc.applicationId)\n",
    "\n",
    "# Return default level of parallelism \n",
    "print(sc.defaultParallelism)\n",
    "\n",
    "# Default minimum number of partitions for RDDs\n",
    "print(sc.defaultMinPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading csv-files into the Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[tailnum: string, year: string, type: string, manufacturer: string, model: string, engines: int, seats: int, speed: string, engine: string]\n"
     ]
    }
   ],
   "source": [
    "path_source = r\"planes.csv\"\n",
    "\n",
    "df_planes = sqlContext.read.load (path_source,\n",
    "                           format='com.databricks.spark.csv',\n",
    "                           header='true',\n",
    "                           inferSchema='true')\n",
    "                           \n",
    "print(df_planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planes_csv\n"
     ]
    }
   ],
   "source": [
    "temp_table_name = \"planes_csv\"\n",
    "print(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_planes.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_flights = r\"flights_small.csv\"\n",
    "\n",
    "df_flights = sqlContext.read.load (src_flights,\n",
    "                           format='com.databricks.spark.csv',\n",
    "                           header='true',\n",
    "                           inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(year,IntegerType,true),StructField(month,IntegerType,true),StructField(day,IntegerType,true),StructField(dep_time,StringType,true),StructField(dep_delay,StringType,true),StructField(arr_time,StringType,true),StructField(arr_delay,StringType,true),StructField(carrier,StringType,true),StructField(tailnum,StringType,true),StructField(flight,IntegerType,true),StructField(origin,StringType,true),StructField(dest,StringType,true),StructField(air_time,StringType,true),StructField(distance,IntegerType,true),StructField(hour,StringType,true),StructField(minute,StringType,true)))\n",
      "\n",
      "df_flights\n"
     ]
    }
   ],
   "source": [
    "# schema delivers the data types of the variables or columns\n",
    "print(df_flights.schema)\n",
    "\n",
    "print(\"\")\n",
    "temp_table_name_2 = \"df_flights\"\n",
    "print(temp_table_name_2)\n",
    "df_flights.createOrReplaceTempView(temp_table_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(faa,StringType,true),StructField(name,StringType,true),StructField(lat,DoubleType,true),StructField(lon,DoubleType,true),StructField(alt,IntegerType,true),StructField(tz,IntegerType,true),StructField(dst,StringType,true)))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_airports =  r\"airports.csv\"\n",
    "\n",
    "df_airports = sqlContext.read.load (src_airports,\n",
    "                           format='com.databricks.spark.csv',\n",
    "                           header='true',\n",
    "                           inferSchema='true')\n",
    "df_airports.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_airports\n"
     ]
    }
   ],
   "source": [
    "temp_table_name_3 = \"df_airports\"\n",
    "print(temp_table_name_3)\n",
    "df_airports.createOrReplaceTempView(temp_table_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if object is a data frame or RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DataFrame'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_df(x):\n",
    "    if isinstance(x, RDD):\n",
    "        return \"RDD\"\n",
    "    if isinstance(x, DataFrame):\n",
    "        return \"DataFrame\"\n",
    "    \n",
    "check_df(df_airports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession.catalog.listTables()\n",
    "<p>\n",
    "The attribute catalog of the active Spark session lists the data \n",
    "inside the cluster. The .listTables() method generates\n",
    "all the tables inside the cluster as a list. \n",
    "This helps to get orientation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='df_airports', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='df_flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='planes_csv', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in csc files with read.csv as data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(tailnum,StringType,true),StructField(year,StringType,true),StructField(type,StringType,true),StructField(manufacturer,StringType,true),StructField(model,StringType,true),StructField(engines,IntegerType,true),StructField(seats,IntegerType,true),StructField(speed,StringType,true),StructField(engine,StringType,true)))\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planes2 = spark.read.csv(\"planes.csv\", inferSchema=True, header=True)\n",
    "print(planes2.schema)\n",
    "print(isinstance(planes2, DataFrame ))\n",
    "\n",
    "print(\"\") \n",
    "\n",
    "flights2 = \\\n",
    "spark.read.csv(\"flights_small.csv\", inferSchema=True, header=True)\n",
    "\n",
    "airports2 = spark.read.csv(\"airports.csv\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'tailnum', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute']\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n",
      "None\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema displays the column types\n",
    "# it has shown that a lot of numeric variables were stored as strings\n",
    "# this needed to be corrected\n",
    "\n",
    "print(flights2.columns)\n",
    "print(\"\")\n",
    "\n",
    "# convert air_time from string to integer\n",
    "# with column creates a new column air_time of type integer\n",
    "flights2 = \\\n",
    "flights2.withColumn(\"air_time\", flights2[\"air_time\"].cast(IntegerType()))\n",
    "\n",
    "# converting multiple columns with a for loop\n",
    "\n",
    "flights_cols = \\\n",
    "['dep_time', 'dep_delay', 'arr_time', 'arr_delay',  'hour', 'minute']\n",
    "\n",
    "\n",
    "for i in flights_cols:\n",
    "    flights2 = flights2.withColumn(i, flights2[i].cast(IntegerType()))\n",
    "    \n",
    "\n",
    "\n",
    "# printSchema displays the column types neatly\n",
    "print(flights2.printSchema())\n",
    "print(isinstance(flights2, DataFrame ))\n",
    "\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 'int'),\n",
       " ('month', 'int'),\n",
       " ('day', 'int'),\n",
       " ('dep_time', 'int'),\n",
       " ('dep_delay', 'int'),\n",
       " ('arr_time', 'int'),\n",
       " ('arr_delay', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('tailnum', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('origin', 'string'),\n",
       " ('dest', 'string'),\n",
       " ('air_time', 'int'),\n",
       " ('distance', 'int'),\n",
       " ('hour', 'int'),\n",
       " ('minute', 'int')]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(tailnum,StringType,true),StructField(year,StringType,true),StructField(type,StringType,true),StructField(manufacturer,StringType,true),StructField(model,StringType,true),StructField(engines,IntegerType,true),StructField(seats,IntegerType,true),StructField(speed,StringType,true),StructField(engine,StringType,true)))\n",
      "True\n",
      "\n",
      "root\n",
      " |-- faa: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- alt: integer (nullable = true)\n",
      " |-- tz: integer (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      "\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(planes2.schema)\n",
    "print(isinstance(planes2, DataFrame ))\n",
    "\n",
    "print(\"\") \n",
    "\n",
    "print(airports2.printSchema())\n",
    "print(isinstance(airports2, DataFrame ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|                type|    manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "| N102UW|1998|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N103US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N104UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|faa|                name|       lat|        lon| alt| tz|dst|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
      "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
      "|06C| Schaumburg Regional|41.9893408|-88.1012428| 801| -6|  A|\n",
      "+---+--------------------+----------+-----------+----+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# show displays the df\n",
    "\n",
    "print(planes2.show(n=3))\n",
    "print(\"\")\n",
    "\n",
    "print(flights2.show(n=3))\n",
    "print(\"\")\n",
    "\n",
    "print(airports2.show(n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing\n",
    "\n",
    "the data for using it into machine learning model. This includes here:\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>joining the tables containing the features or variables used as predictors,</li>\n",
    "<li>convert data types from type object or string to a \n",
    "unambiguous numeric,</li>\n",
    "<li>generating new features from existing variables,</li>\n",
    "<li>creating the target variables, which should be predicted,</li>        \n",
    "<li>convert data types from type object or string to a \n",
    "unambiguous numeric,</li>\n",
    "<li>encoding \n",
    "string variables with numeric as Spark can only process numerics, </li>\n",
    "<li> incorporate everything into a pipeline object for smooth execution\n",
    "in the model.</li>\n",
    "\n",
    "</ul> \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tailnum', 'year', 'type', 'manufacturer', 'model', 'engines', 'seats', 'speed', 'engine']\n",
      "9\n",
      "\n",
      "['year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'tailnum', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute']\n",
      "16\n",
      "\n",
      "Number of joined columns:  24\n"
     ]
    }
   ],
   "source": [
    "# the goal is to join the flights and the planes table\n",
    "print(planes2.columns)\n",
    "print(len(planes2.columns))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(flights2.columns)\n",
    "print(len(flights2.columns))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Number of joined columns: \", 9+16-1)\n",
    "\n",
    "# year is in both df and without renamimg duplicate column names will \n",
    "# occur after joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tailnum', 'planes_year', 'type', 'manufacturer', 'model', 'engines', 'seats', 'speed', 'engine']\n"
     ]
    }
   ],
   "source": [
    "planes3 = planes2.withColumnRenamed('year', 'planes_year')\n",
    "print(planes3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of joined columns:  24\n"
     ]
    }
   ],
   "source": [
    "# tailnum is the key variable\n",
    "joined_1 = flights2.join(planes3, on='tailnum', how='leftouter')\n",
    "\n",
    "print(\"Number of joined columns: \", len(joined_1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dest', 'name', 'lat', 'lon', 'alt', 'tz', 'dst']\n",
      "+-------+--------------------+---------+-----------+----+--------+\n",
      "|tailnum|                name|      lat|        lon|dest|air_time|\n",
      "+-------+--------------------+---------+-----------+----+--------+\n",
      "| N846VA|    Los Angeles Intl|33.942536|-118.408075| LAX|     132|\n",
      "| N559AS|       Honolulu Intl|21.318681|-157.922428| HNL|     360|\n",
      "| N847VA|  San Francisco Intl|37.618972|-122.374889| SFO|     111|\n",
      "| N360SW|Norman Y Mineta S...|  37.3626|-121.929022| SJC|      83|\n",
      "| N612AS|            Bob Hope|34.200667|-118.358667| BUR|     127|\n",
      "+-------+--------------------+---------+-----------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['dest', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'tailnum', 'flight', 'origin', 'air_time', 'distance', 'hour', 'minute', 'name', 'lat', 'lon', 'alt', 'tz', 'dst']\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# the goal is to join the flights and the aiports table\n",
    "airports2.columns\n",
    "\n",
    "airports3 = airports2.withColumnRenamed('faa' , 'dest')\n",
    "# faa = old name, dest = new name\n",
    "print(airports3.columns)\n",
    "\n",
    "join_2 = flights2.join(airports3, on=\"dest\", how='leftouter')\n",
    "# left table = flights2, right table = aiports3\n",
    "\n",
    "join_2.\\\n",
    "select('tailnum', 'name', 'lat', 'lon', 'dest', 'air_time').show(5)\n",
    "\n",
    "print(join_2.columns)\n",
    "print(len(join_2.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join 3\n",
    "\n",
    "Before it was not possible to join aiports with planes\n",
    "for lack of common key. As now tailnum is in Join 2 it can be joined \n",
    "with planes. If there is not immediate common key to join\n",
    "sometimes such a detour is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tailnum', 'dest', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'flight', 'origin', 'air_time', 'distance', 'hour', 'minute', 'name', 'lat', 'lon', 'alt', 'tz', 'dst', 'year', 'type', 'manufacturer', 'model', 'engines', 'seats', 'speed', 'engine']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "join_3 = join_2.join(planes2, on='tailnum', how='inner')\n",
    "\n",
    "print(join_3.columns)\n",
    "print(len(join_3.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data types \n",
    "\n",
    "<p>\n",
    "to numeric values (integers or doubles) if it makes sense as Spark can process only numeric values. One reason this is needed is because\n",
    "Spark does not grasp the data type always correctly with infer_schema,\n",
    "when loading the data into the cluster.\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- planes_year: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engines: integer (nullable = true)\n",
      " |-- seats: integer (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tailnum', 'year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute', 'planes_year', 'type', 'manufacturer', 'model', 'engines', 'seats', 'speed', 'engine']\n"
     ]
    }
   ],
   "source": [
    "print(joined_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_1 = ['dep_time', 'dep_delay', 'arr_time', 'arr_delay', \n",
    "          'air_time', 'distance', 'planes_year','speed']\n",
    "\n",
    "for i in cols_1:\n",
    "    joined_1 = joined_1.withColumn(i, joined_1[i].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- planes_year: integer (nullable = true)\n",
      " |-- speed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_1.select(cols_1).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a new variable from existing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- plane_years: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- planes_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Years the plane is used in years\n",
    "joined_2 = joined_1.withColumn('plane_years', \n",
    "                               joined_1.year - joined_1.planes_year)\n",
    "\n",
    "joined_2.select('plane_years', 'year', 'planes_year').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new binary boolean target or label variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|late_departure|\n",
      "+--------------+\n",
      "|         false|\n",
      "|          true|\n",
      "|         false|\n",
      "|          true|\n",
      "|         false|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Is late departure occuring?\n",
    "joined_3_dep = \\\n",
    "joined_2.withColumn('late_departure', joined_2.dep_delay > 0)\n",
    "\n",
    "joined_3_dep.select('late_departure').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "joined_3_dep_b = \\\n",
    "joined_3_dep.withColumn('label', joined_3_dep['late_departure'].\\\n",
    "                        cast('integer'))\n",
    "\n",
    "print(joined_3_dep_b.select('label').show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|late_arrival|\n",
      "+------------+\n",
      "|       false|\n",
      "|        true|\n",
      "|        true|\n",
      "|        true|\n",
      "|        true|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Is late arrival occurring?\n",
    "joined_3_arr = \\\n",
    "joined_2.withColumn('late_arrival', joined_2.arr_delay > 0)\n",
    "\n",
    "joined_3_arr.select('late_arrival').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    1|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark processes numeric data\n",
    "# the boolean string are replaced with binary 1 and 0\n",
    "joined_3_arr_b = \\\n",
    "joined_3_arr.withColumn('label', joined_3_arr['late_arrival'].\n",
    "                        cast('integer'))\n",
    "\n",
    "joined_3_arr_b.select('label').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(distance)|\n",
      "+-------------+\n",
      "|         2724|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|           93|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "+-------------+\n",
      "|long_distance|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Is the flight long distance or above 1000?\n",
    "joined_2.select([max(\"distance\")]).show()\n",
    "joined_2.select([min(\"distance\")]).show()\n",
    "print(\"\")\n",
    "\n",
    "joined_4_dist = \\\n",
    "joined_2.withColumn('long_distance', joined_2.distance > 1000)\n",
    "\n",
    "joined_4_dist.select('long_distance').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_4_dist_b = \\\n",
    "joined_4_dist.withColumn('label', joined_4_dist['long_distance'].\n",
    "                         cast('integer'))\n",
    "\n",
    "joined_4_dist_b.select('label').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values?\n",
    "\n",
    "Checking if missing values are given in the variables\n",
    "of the data set utilized for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations:  10000\n",
      "\n",
      "isNotNull of variable arr_delay : 9925\n",
      "isNUll of variable arr_delay 75\n",
      "isNotNull of variable dep_delay : 9952\n",
      "isNUll of variable dep_delay 48\n",
      "isNotNull of variable plane_years : 9354\n",
      "isNUll of variable plane_years 646\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations: \", joined_3_arr_b.count())\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "check_null_list = [\"arr_delay\", \"dep_delay\", \"plane_years\"]\n",
    "\n",
    "for i in check_null_list:\n",
    "    n_notnull = \\\n",
    "    joined_3_arr_b.filter(joined_3_arr_b[i].isNotNull()).count()\n",
    "    \n",
    "    print(\"isNotNull of variable\" , i, \":\", n_notnull)\n",
    "    \n",
    "    n_observations = joined_3_arr_b.count()\n",
    "    \n",
    "    n_null = n_observations - n_notnull\n",
    "    \n",
    "    print(\"isNUll of variable\", i, n_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the for loop above into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null(cols, df):\n",
    "    for i in cols:\n",
    "            n_notnull = \\\n",
    "            df.filter(df[i].isNotNull()).count()\n",
    "\n",
    "            print(\"isNotNull of variable\" , i, \":\", n_notnull)\n",
    "\n",
    "            n_observations = df.count()\n",
    "\n",
    "            n_null = n_observations - n_notnull\n",
    "\n",
    "            print(\"isNUll of variable\", i, n_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isNotNull of variable arr_delay : 9925\n",
      "isNUll of variable arr_delay 75\n",
      "isNotNull of variable dep_delay : 9952\n",
      "isNUll of variable dep_delay 48\n",
      "isNotNull of variable plane_years : 9354\n",
      "isNUll of variable plane_years 646\n"
     ]
    }
   ],
   "source": [
    "# test if function works\n",
    "# works\n",
    "check_null(cols=check_null_list , df=joined_3_arr_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined3c = joined_3_arr_b.na.drop(subset=check_null_list)\n",
    "len(joined3c.columns)\n",
    "# All columns are remaining, drop did not interfer with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isNotNull of variable arr_delay : 9303\n",
      "isNUll of variable arr_delay 0\n",
      "isNotNull of variable dep_delay : 9303\n",
      "isNUll of variable dep_delay 0\n",
      "isNotNull of variable plane_years : 9303\n",
      "isNUll of variable plane_years 0\n"
     ]
    }
   ],
   "source": [
    "check_null(cols=check_null_list, df=joined3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isNotNull of variable arr_delay : 9303\n",
      "isNUll of variable arr_delay 0\n",
      "isNotNull of variable dep_delay : 9303\n",
      "isNUll of variable dep_delay 0\n",
      "isNotNull of variable plane_years : 9303\n",
      "isNUll of variable plane_years 0\n"
     ]
    }
   ],
   "source": [
    "joined1c = joined_3_dep_b.na.drop(subset=check_null_list) \n",
    "check_null(cols=check_null_list, df=joined1c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isNotNull of variable arr_delay : 9303\n",
      "isNUll of variable arr_delay 0\n",
      "isNotNull of variable dep_delay : 9303\n",
      "isNUll of variable dep_delay 0\n",
      "isNotNull of variable plane_years : 9303\n",
      "isNUll of variable plane_years 0\n"
     ]
    }
   ],
   "source": [
    "joined4c = joined_4_dist_b.na.drop(subset=check_null_list) \n",
    "check_null(cols=check_null_list, df=joined4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding & Pipelines\n",
    "\n",
    "<p>\n",
    "transforming string variables with a limited number of categories and\n",
    "make it ready for execution with one pipeline object.\n",
    "In Pyspark the subsequent methods are applied sequentially.\n",
    "</p> \n",
    "\n",
    "\n",
    " <ul>\n",
    "  <li><b>StringIndexer</b>: maps by estimating and then transforming \n",
    "      each unique string value of the data frame to a number. This is comparable to label encoding in scikit for example.\n",
    "  </li>\n",
    "  <li><b>OneHotEncoder (OHE)</b>: The output of the StringIndexer \n",
    "      is the input into the OHE. The OHE creates a vector of possible\n",
    "      values over given observations. A 1 represents the actual given\n",
    "      value, whereas a 0 indicates this is not the value of the variable.\n",
    "  </li>\n",
    "  <li><b>VectorAssembler</b>: As explained above OHE creates a vector from\n",
    "      variable, where all values are 0 and but one. This increases the\n",
    "      number of columns. Many OHE vectors are incoporated into the\n",
    "      data frame with the VectorAssembler class as new columns.\n",
    "  </li>\n",
    "  <li><b>Pipeline</b>: as also used in Scikit for example binds every\n",
    "         estimator and transformer object created beforehand into one \n",
    "         one object. This can then be executed in sequential steps. \n",
    "         More on pipelines in the\n",
    "         <a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html\"\n",
    "            target=\"_blank\">doc</a>.\n",
    "    </li>\n",
    "</ul> \n",
    "\n",
    "<p>This process is shown below.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The String Indexer creates a number for every unique string value.\n",
    "# This is comparable to the label encoder in scikit\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.htmlwqassasay\n",
    "\n",
    "origin_indexer = \\\n",
    "StringIndexer(inputCol=\"origin\", outputCol=\"origin_index\")\n",
    "\n",
    "# The output of the String Indexer is the input to the one hot encoder\n",
    "# This is carried out by the pipeline later\n",
    "\n",
    "origin_encoder = \\\n",
    "OneHotEncoder(inputCol=\"origin_index\", outputCol=\"origin_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "carr_indexer = \\\n",
    "StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = \\\n",
    "OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol=\"dest\", \n",
    "outputCol=\"dest_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\",\n",
    "outputCol=\"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a VectorAssembler collecting the features\n",
    "# and take in the beforehand encoded variables\n",
    "# and others features\n",
    "# used later in the data model\n",
    "\n",
    "vec_assembler = \\\n",
    "VectorAssembler(inputCols=[\"month\", \"air_time\", \"origin_fact\", \n",
    "                            \"carrier_fact\", \"dest_fact\", \"plane_years\"], \n",
    "                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_c97f2d1d88db"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Pipeline\n",
    "# Make the pipeline\n",
    "airtransport_pipeline = \\\n",
    "Pipeline(stages=[origin_indexer, origin_encoder,\n",
    "                 dest_indexer, dest_encoder,\n",
    "                 carr_indexer, carr_encoder, \n",
    "                 vec_assembler])\n",
    "\n",
    "airtransport_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estimator is an algorithm, which can be fit on a dataframe to fit a \n",
    "transformer. A transformer \"transforms\" a dataframe into another enhanced\n",
    "data frame (from features to predictions for example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_into_pipe = airtransport_pipeline.fit(joined3c).transform(joined3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and a test set\n",
    "\n",
    "<p>\n",
    "is a core data science technique to prevent overfitting of the model\n",
    "on the (train) data and decreasing performance on new data (real world).\n",
    "Iterating through the training data with cross-validation-methods\n",
    "can improve the performance and validitdy of the model further.\n",
    "The final check with the test set (or holdout set, when the data was \n",
    "never used before in the process give an realistic idea of the model\n",
    "performance in the \"real world\".\n",
    "</p> \n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets\" target=\"_blank\">For more info compare Wikipedia</a> \n",
    "\n",
    "<p>\n",
    "Here only simple train-test-splits are used.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_1, test_1 = dt_into_pipe.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6524, 34)\n",
      "(2779, 34)\n"
     ]
    }
   ],
   "source": [
    "print((training_1.count(), len(training_1.columns)))\n",
    "print((test_1.count(), len(test_1.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tailnum',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dep_time',\n",
       " 'dep_delay',\n",
       " 'arr_time',\n",
       " 'arr_delay',\n",
       " 'carrier',\n",
       " 'flight',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'air_time',\n",
       " 'distance',\n",
       " 'hour',\n",
       " 'minute',\n",
       " 'planes_year',\n",
       " 'type',\n",
       " 'manufacturer',\n",
       " 'model',\n",
       " 'engines',\n",
       " 'seats',\n",
       " 'speed',\n",
       " 'engine',\n",
       " 'plane_years',\n",
       " 'late_arrival',\n",
       " 'label',\n",
       " 'origin_index',\n",
       " 'origin_fact',\n",
       " 'dest_index',\n",
       " 'dest_fact',\n",
       " 'carrier_index',\n",
       " 'carrier_fact',\n",
       " 'features']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the flight late? <br> Answer with predictions generated by different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_acd133027103"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a LogisticRegression Estimator to fit on the df\n",
    "logreg = LogisticRegression(featuresCol=\"features\", labelCol = 'label')\n",
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation and hyper parameter tuning\n",
    "\n",
    "<p>\n",
    "is used to increase the model performance and decrease the\n",
    "model error. In cross validation the data set is split into\n",
    "different data partitions a number of times. \n",
    "For every subset or fold the majority of partitions is used for training and the remaining block is used as validation (testing) data. Every partition is used once as validation data. Applying an algorithm like logistic regression generates predictions evaluated with a metric\n",
    "like the model error for every subset. \n",
    "</p>     \n",
    "<p>\n",
    "Averaging the metrics\n",
    "gives a good estimation of the actual error. Using this together\n",
    "with hyper parameter tuning, whereby given parameters of the algorithm \n",
    "are provided with a list of values, allows to find and select the best model with the lowest model error. The selected model is finally evaluated \n",
    "with the test data. \n",
    "</p>  \n",
    "<p>\n",
    "Fortunately Apache Pyspark provides a workflow\n",
    "to do all of this similar to Scikit, so there is not a need to code this\n",
    "from the start. It is helpful to keep in mind that cross-validation is\n",
    "a highly computing intensive process. Pyspark running on remote machines\n",
    "in the cloud could manage this process better than a single machine like \n",
    "here.\n",
    "</p> \n",
    "\n",
    "<p> \n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"Cross-validation: evaluating estimator performance\" \n",
    "height=\"250\" width=\"450\"> \n",
    "</p>\n",
    "\n",
    "<p> \n",
    "<div align=\"right\">\n",
    "<a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\" target=\"_blank\">Scikit: Cross-validation: evaluating estimator performance</a> \n",
    "</div> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RandomForestClassifier as a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(training_1)\n",
    "predictions = rfModel.transform(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "printout = [ 'features', 'probability', 'label', 'prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|            features|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|(82,[0,1,2,8,46,8...|[0.61853808920144...|    1|       0.0|\n",
      "|(82,[0,1,8,46,81]...|[0.65090958909260...|    1|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.56267528148207...|    1|       0.0|\n",
      "|(82,[0,1,2,8,46,8...|[0.63558665243240...|    0|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.61134811849834...|    0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(printout).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -5387697053847413545)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model within cross validation\n",
    "\n",
    "<p>\n",
    "A logistic regression model predicts certain outcomes \n",
    "with a probability between 0 and 1. In boolean Terms a probability\n",
    "above a choosen cut-off-point is then True or Yes and below False or No.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation module of the pyspark machine leraning library\n",
    "# It is able to evaluate a range of models created with \n",
    "# linear regression, classification, support vector machines ...\n",
    "\n",
    "# The BinaryClassificationEvaluator uses the area under the curve metric\n",
    "# based on confusion matrix of true positives, false positives ...\n",
    "\n",
    "evaluation_instance_log = \\\n",
    "evaluation.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which hyperparameters can be hyper tuned?\n",
    "<br>An overview is given here:\n",
    " \n",
    "\n",
    "<p>\n",
    "class pyspark.ml.classification.LogisticRegression(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, threshold=0.5, thresholds=None, probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None, aggregationDepth=2, family=\"auto\")\n",
    "</p> \n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\" target=\"_blank\">pyspark.ml package</a> \n",
    "\n",
    "\n",
    "<p>\n",
    "The params use here are: maxIter, elasticNetParam, regParam. \n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamGridBuilder delivers the grid of tuned values\n",
    "grid_log = tune.ParamGridBuilder()\n",
    "\n",
    "# this fills in the values to run into grid log\n",
    "grid_log = grid_log.addGrid(logreg.regParam, np.arange(0, .1, .02))\n",
    "grid_log = grid_log.addGrid(logreg.elasticNetParam, [0,1])\n",
    "grid_log = grid_log.addGrid(logreg.maxIter, [100, 200])\n",
    "\n",
    "# tell pyspark to build the grid\n",
    "grid_log = grid_log.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_log = tune.CrossValidator(estimator=logreg,\n",
    "               estimatorParamMaps=grid_log,\n",
    "               evaluator=evaluation_instance_log\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 104.029 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "# Fit the model logreg to the training data\n",
    "best_model_logreg = cv_log.fit(training_1)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_6d44d455be53\n"
     ]
    }
   ],
   "source": [
    "print(best_model_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|            features|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|(82,[0,1,2,8,46,8...|[0.44012009952712...|    1|       1.0|\n",
      "|(82,[0,1,8,46,81]...|[0.57344578108556...|    1|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.33496975534556...|    1|       1.0|\n",
      "|(82,[0,1,2,8,46,8...|[0.76282840824656...|    0|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.83695555021854...|    0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_model_logreg.transform(test_1)\n",
    "\n",
    "# look into the predictions\n",
    "test_results.select(printout).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve - AUC:  0.7004728340352523\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the predictions\n",
    "print(\"Area under the curve - AUC: \", \n",
    "      evaluation_instance_log.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=0), Row(label=0), Row(label=0), Row(label=0), Row(label=1), Row(label=0), Row(label=0), Row(label=0), Row(label=0), Row(label=1), Row(label=0), Row(label=1), Row(label=0), Row(label=0), Row(label=0)]\n"
     ]
    }
   ],
   "source": [
    "# look into the predictions - alternatively\n",
    "prediction = best_model_logreg.transform(test_1)\n",
    "selected = prediction.select('label')\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for row in selected.collect():\n",
    "    predicted_labels.append(row)\n",
    "    \n",
    "print(predicted_labels[100:115])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model within cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# create contingency table or confustion matrix\n",
    "# https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "evaluator_rf = MulticlassClassificationEvaluator() \n",
    "evaluator_rf_2 = \\\n",
    "evaluation.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables\n",
    "RANDOM_SEED = 302\n",
    "RF_NUM_TREES = [30, 60, 90]\n",
    "RF_MAX_DEPTH = [4,6]\n",
    "RF_NUM_BINS = [32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamGridBuilder delivers the grid of tuned values\n",
    "grid_rf = tune.ParamGridBuilder()\n",
    "\n",
    "# this fills in the values to run into grid log\n",
    "grid_rf = grid_rf.addGrid(model_rf.maxBins, RF_NUM_BINS)\n",
    "grid_rf = grid_rf.addGrid(model_rf.maxDepth, RF_MAX_DEPTH)\n",
    "grid_rf = grid_rf.addGrid(model_rf.numTrees, RF_NUM_BINS)\n",
    "\n",
    "# tell pyspark to build the grid\n",
    "grid_rf = grid_rf.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validator takes inputs: model, grid, evaluation\n",
    "cv_rf = tune.CrossValidator(estimator=model_rf,\n",
    "                             estimatorParamMaps=grid_rf,\n",
    "                             evaluator=evaluator_rf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 55.616 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "# Fit the model logreg to the training data\n",
    "best_model_rf = cv_rf.fit(training_1)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier_9eea922fe729__maxBins\n",
      "RandomForestClassifier_9eea922fe729__maxDepth\n"
     ]
    }
   ],
   "source": [
    "bestModel = best_model_rf.bestModel\n",
    "\n",
    "bestParams = bestModel.extractParamMap()\n",
    "\n",
    "# bestParams.keys()\n",
    "print(bestModel.getParam('maxBins'))\n",
    "print(bestModel.getParam('maxDepth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|            features|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|(82,[0,1,2,8,46,8...|[0.63114203042744...|    1|       0.0|\n",
      "|(82,[0,1,8,46,81]...|[0.65056148774525...|    1|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.62100497594102...|    1|       0.0|\n",
      "|(82,[0,1,2,8,46,8...|[0.63930185146379...|    0|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.64834766347528...|    0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results_rf = best_model_rf.transform(test_1)\n",
    "\n",
    "# look into the predictions\n",
    "test_results_rf.select(printout).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve - AUC:  0.599337482543627\n",
      "f1:  0.4769752475538342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the predictions\n",
    "print(\"Area under the curve - AUC: \", \n",
    "      evaluator_rf_2.evaluate(test_results_rf))\n",
    "\n",
    "#  \tmetricName() param for metric name in evaluation \n",
    "# (supports \"f1\" (default), \"weightedPrecision\", \"weightedRecall\", \n",
    "# \"accuracy\")\n",
    "# https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/ml/evaluation/MulticlassClassificationEvaluator.html\n",
    "print(\"f1: \", evaluator_rf.evaluate(test_results_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-Boosted Tree Classifier\n",
    "\n",
    "<p>produces normally good results and just for fun a simple \n",
    "model is tested here.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 6.912 seconds\n"
     ]
    }
   ],
   "source": [
    "# create Gradient-Boosted Tree Classifier instance \n",
    "# and fit it into data\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# Fit the model logreg to the training data\n",
    "gbtModel = gbt.fit(training_1)\n",
    "\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+----------+\n",
      "|            features|         probability|label|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|(82,[0,1,2,8,46,8...|[0.52071097957816...|    1|       0.0|\n",
      "|(82,[0,1,8,46,81]...|[0.65573352669052...|    1|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.52006585456176...|    1|       0.0|\n",
      "|(82,[0,1,2,8,46,8...|[0.66444710531219...|    0|       0.0|\n",
      "|(82,[0,1,2,8,45,8...|[0.66444710531219...|    0|       0.0|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# generate data frame with predictions\n",
    "predictions_gbt = gbtModel.transform(test_1)\n",
    "print(predictions_gbt.select(printout).show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve - AUC:0.6036257023784655\n"
     ]
    }
   ],
   "source": [
    "evaluator_gbt = BinaryClassificationEvaluator()\n",
    "print(\"Area under the curve - AUC:\" \n",
    "      + str(evaluator_gbt.evaluate(predictions, \n",
    "     {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model within cross validation produces the best result\n",
    "in terms of AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -5387697053847413545)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# a nice feature is explainParams\n",
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
